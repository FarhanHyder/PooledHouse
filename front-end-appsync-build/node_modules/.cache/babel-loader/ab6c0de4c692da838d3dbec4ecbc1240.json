{"ast":null,"code":"// Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n// Permission is hereby granted, free of charge, to any person obtaining a copy of this\n// software and associated documentation files (the \"Software\"), to deal in the Software\n// without restriction, including without limitation the rights to use, copy, modify,\n// merge, publish, distribute, sublicense, and/or sell copies of the Software, and to\n// permit persons to whom the Software is furnished to do so.\n// THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED,\n// INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A\n// PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT\n// HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION\n// OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\n// SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n(function e(t, n, r) {\n  function s(o, u) {\n    if (!n[o]) {\n      if (!t[o]) {\n        var a = typeof require == \"function\" && require;\n        if (!u && a) return a(o, !0);\n        if (i) return i(o, !0);\n        var f = new Error(\"Cannot find module '\" + o + \"'\");\n        throw f.code = \"MODULE_NOT_FOUND\", f;\n      }\n\n      var l = n[o] = {\n        exports: {}\n      };\n      t[o][0].call(l.exports, function (e) {\n        var n = t[o][1][e];\n        return s(n ? n : e);\n      }, l, l.exports, e, t, n, r);\n    }\n\n    return n[o].exports;\n  }\n\n  var i = typeof require == \"function\" && require;\n\n  for (var o = 0; o < r.length; o++) {\n    s(r[o]);\n  }\n\n  return s;\n})({\n  1: [function (require, module, exports) {\n    (function () {\n      'use strict';\n\n      var rec = require('./recorder.js');\n\n      var recorder,\n          audioRecorder,\n          checkAudioSupport,\n          audioSupported,\n          playbackSource,\n          UNSUPPORTED = 'Audio is not supported.';\n      /**\n       * Represents an audio control that can start and stop recording,\n       * export captured audio, play an audio buffer, and check if audio\n       * is supported.\n       */\n\n      exports.audioControl = function (options) {\n        options = options || {};\n        this.checkAudioSupport = options.checkAudioSupport !== false;\n        /**\n         * This callback type is called `onSilenceCallback`.\n         *\n         * @callback onSilenceCallback\n         */\n\n        /**\n         * Visualize callback: `visualizerCallback`.\n         *\n         * @callback visualizerCallback\n         * @param {Uint8Array} dataArray\n         * @param {number} bufferLength\n         */\n\n        /**\n         * Clears the previous buffer and starts buffering audio.\n         *\n         * @param {?onSilenceCallback} onSilence - Called when silence is detected.\n         * @param {?visualizerCallback} visualizer - Can be used to visualize the captured buffer.\n         * @param {silenceDetectionConfig} - Specify custom silence detection values.\n         * @throws {Error} If audio is not supported.\n         */\n\n        var startRecording = function startRecording(onSilence, visualizer, silenceDetectionConfig) {\n          onSilence = onSilence || function () {};\n\n          visualizer = visualizer || function () {};\n\n          audioSupported = audioSupported !== false;\n\n          if (!audioSupported) {\n            throw new Error(UNSUPPORTED);\n          }\n\n          recorder = audioRecorder.createRecorder(silenceDetectionConfig);\n          recorder.record(onSilence, visualizer);\n        };\n        /**\n         * Stops buffering audio.\n         *\n         * @throws {Error} If audio is not supported.\n         */\n\n\n        var stopRecording = function stopRecording() {\n          audioSupported = audioSupported !== false;\n\n          if (!audioSupported) {\n            throw new Error(UNSUPPORTED);\n          }\n\n          recorder.stop();\n        };\n        /**\n         * On export complete callback: `onExportComplete`.\n         *\n         * @callback onExportComplete\n         * @param {Blob} blob The exported audio as a Blob.\n         */\n\n        /**\n         * Exports the captured audio buffer.\n         *\n         * @param {onExportComplete} callback - Called when the export is complete.\n         * @param {sampleRate} The sample rate to use in the export.\n         * @throws {Error} If audio is not supported.\n         */\n\n\n        var exportWAV = function exportWAV(callback, sampleRate) {\n          audioSupported = audioSupported !== false;\n\n          if (!audioSupported) {\n            throw new Error(UNSUPPORTED);\n          }\n\n          if (!(callback && typeof callback === 'function')) {\n            throw new Error('You must pass a callback function to export.');\n          }\n\n          sampleRate = typeof sampleRate !== 'undefined' ? sampleRate : 16000;\n          recorder.exportWAV(callback, sampleRate);\n          recorder.clear();\n        };\n        /**\n         * On playback complete callback: `onPlaybackComplete`.\n         *\n         * @callback onPlaybackComplete\n         */\n\n        /**\n         * Plays the audio buffer with an HTML5 audio tag.\n         * @param {Uint8Array} buffer - The audio buffer to play.\n         * @param {?onPlaybackComplete} callback - Called when audio playback is complete.\n         */\n\n\n        var playHtmlAudioElement = function playHtmlAudioElement(buffer, callback) {\n          if (typeof buffer === 'undefined') {\n            return;\n          }\n\n          var myBlob = new Blob([buffer]);\n          var audio = document.createElement('audio');\n          var objectUrl = window.URL.createObjectURL(myBlob);\n          audio.src = objectUrl;\n          audio.addEventListener('ended', function () {\n            audio.currentTime = 0;\n\n            if (typeof callback === 'function') {\n              callback();\n            }\n          });\n          audio.play();\n        };\n        /**\n         * On playback complete callback: `onPlaybackComplete`.\n         *\n         * @callback onPlaybackComplete\n         */\n\n        /**\n         * Plays the audio buffer with a WebAudio AudioBufferSourceNode.\n         * @param {Uint8Array} buffer - The audio buffer to play.\n         * @param {?onPlaybackComplete} callback - Called when audio playback is complete.\n         */\n\n\n        var play = function play(buffer, callback) {\n          if (typeof buffer === 'undefined') {\n            return;\n          }\n\n          var myBlob = new Blob([buffer]); // We'll use a FileReader to create and ArrayBuffer out of the audio response.\n\n          var fileReader = new FileReader();\n\n          fileReader.onload = function () {\n            // Once we have an ArrayBuffer we can create our BufferSource and decode the result as an AudioBuffer.\n            playbackSource = audioRecorder.audioContext().createBufferSource();\n            audioRecorder.audioContext().decodeAudioData(this.result, function (buf) {\n              // Set the source buffer as our new AudioBuffer.\n              playbackSource.buffer = buf; // Set the destination (the actual audio-rendering device--your device's speakers).\n\n              playbackSource.connect(audioRecorder.audioContext().destination); // Add an \"on ended\" callback.\n\n              playbackSource.onended = function (event) {\n                if (typeof callback === 'function') {\n                  callback();\n                }\n              }; // Start the playback.\n\n\n              playbackSource.start(0);\n            });\n          };\n\n          fileReader.readAsArrayBuffer(myBlob);\n        };\n        /**\n         * Stops the playback source (created by the play method) if it exists. The `onPlaybackComplete`\n         * callback will be called.\n         */\n\n\n        var stop = function stop() {\n          if (typeof playbackSource === 'undefined') {\n            return;\n          }\n\n          playbackSource.stop();\n        };\n        /**\n         * Clear the recording buffer.\n         */\n\n\n        var clear = function clear() {\n          recorder.clear();\n        };\n        /**\n         * On audio supported callback: `onAudioSupported`.\n         *\n         * @callback onAudioSupported\n         * @param {boolean}\n         */\n\n        /**\n         * Checks that getUserMedia is supported and the user has given us access to the mic.\n         * @param {onAudioSupported} callback - Called with the result.\n         */\n\n\n        var supportsAudio = function supportsAudio(callback) {\n          callback = callback || function () {};\n\n          if (navigator.mediaDevices && navigator.mediaDevices.getUserMedia) {\n            audioRecorder = rec.audioRecorder();\n            audioRecorder.requestDevice().then(function (stream) {\n              audioSupported = true;\n              callback(audioSupported);\n            }).catch(function (error) {\n              audioSupported = false;\n              callback(audioSupported);\n            });\n          } else {\n            audioSupported = false;\n            callback(audioSupported);\n          }\n        };\n\n        if (this.checkAudioSupport) {\n          supportsAudio();\n        }\n\n        return {\n          startRecording: startRecording,\n          stopRecording: stopRecording,\n          exportWAV: exportWAV,\n          play: play,\n          stop: stop,\n          clear: clear,\n          playHtmlAudioElement: playHtmlAudioElement,\n          supportsAudio: supportsAudio\n        };\n      };\n    })();\n  }, {\n    \"./recorder.js\": 5\n  }],\n  2: [function (require, module, exports) {\n    (function () {\n      'use strict';\n\n      var AudioControl = require('./control.js').audioControl;\n\n      var DEFAULT_LATEST = '$LATEST';\n      var DEFAULT_CONTENT_TYPE = 'audio/x-l16; sample-rate=16000';\n      var DEFAULT_USER_ID = 'userId';\n      var DEFAULT_ACCEPT_HEADER_VALUE = 'audio/mpeg';\n      var MESSAGES = Object.freeze({\n        PASSIVE: 'Passive',\n        LISTENING: 'Listening',\n        SENDING: 'Sending',\n        SPEAKING: 'Speaking'\n      });\n      var lexruntime,\n          audioControl = new AudioControl({\n        checkAudioSupport: false\n      });\n\n      exports.conversation = function (config, onStateChange, onSuccess, onError, onAudioData) {\n        var currentState; // Apply default values.\n\n        this.config = applyDefaults(config);\n        this.lexConfig = this.config.lexConfig;\n        this.messages = MESSAGES;\n\n        onStateChange = onStateChange || function () {};\n\n        this.onSuccess = onSuccess || function () {};\n\n        this.onError = onError || function () {};\n\n        this.onAudioData = onAudioData || function () {}; // Validate input.\n\n\n        if (!this.config.lexConfig.botName) {\n          this.onError('A Bot name must be provided.');\n          return;\n        }\n\n        if (!AWS.config.credentials) {\n          this.onError('AWS Credentials must be provided.');\n          return;\n        }\n\n        if (!AWS.config.region) {\n          this.onError('A Region value must be provided.');\n          return;\n        }\n\n        lexruntime = new AWS.LexRuntime();\n\n        this.onSilence = function () {\n          if (config.silenceDetection) {\n            audioControl.stopRecording();\n            currentState.advanceConversation();\n          }\n        };\n\n        this.transition = function (conversation) {\n          currentState = conversation;\n          var state = currentState.state;\n          onStateChange(state.message); // If we are transitioning into SENDING or SPEAKING we want to immediately advance the conversation state\n          // to start the service call or playback.\n\n          if (state.message === state.messages.SENDING || state.message === state.messages.SPEAKING) {\n            currentState.advanceConversation();\n          } // If we are transitioning in to sending and we are not detecting silence (this was a manual state change)\n          // we need to do some cleanup: stop recording, and stop rendering.\n\n\n          if (state.message === state.messages.SENDING && !this.config.silenceDetection) {\n            audioControl.stopRecording();\n          }\n        };\n\n        this.advanceConversation = function () {\n          audioControl.supportsAudio(function (supported) {\n            if (supported) {\n              currentState.advanceConversation();\n            } else {\n              onError('Audio is not supported.');\n            }\n          });\n        };\n\n        this.updateConfig = function (newValue) {\n          this.config = applyDefaults(newValue);\n          this.lexConfig = this.config.lexConfig;\n        };\n\n        this.reset = function () {\n          audioControl.clear();\n          currentState = new Initial(currentState.state);\n        };\n\n        currentState = new Initial(this);\n        return {\n          advanceConversation: this.advanceConversation,\n          updateConfig: this.updateConfig,\n          reset: this.reset\n        };\n      };\n\n      var Initial = function Initial(state) {\n        this.state = state;\n        state.message = state.messages.PASSIVE;\n\n        this.advanceConversation = function () {\n          audioControl.startRecording(state.onSilence, state.onAudioData, state.config.silenceDetectionConfig);\n          state.transition(new Listening(state));\n        };\n      };\n\n      var Listening = function Listening(state) {\n        this.state = state;\n        state.message = state.messages.LISTENING;\n\n        this.advanceConversation = function () {\n          audioControl.exportWAV(function (blob) {\n            state.audioInput = blob;\n            state.transition(new Sending(state));\n          });\n        };\n      };\n\n      var Sending = function Sending(state) {\n        this.state = state;\n        state.message = state.messages.SENDING;\n\n        this.advanceConversation = function () {\n          state.lexConfig.inputStream = state.audioInput;\n          lexruntime.postContent(state.lexConfig, function (err, data) {\n            if (err) {\n              state.onError(err);\n              state.transition(new Initial(state));\n            } else {\n              state.audioOutput = data;\n              state.transition(new Speaking(state));\n              state.onSuccess(data);\n            }\n          });\n        };\n      };\n\n      var Speaking = function Speaking(state) {\n        this.state = state;\n        state.message = state.messages.SPEAKING;\n\n        this.advanceConversation = function () {\n          if (state.audioOutput.contentType === 'audio/mpeg') {\n            audioControl.play(state.audioOutput.audioStream, function () {\n              if (state.audioOutput.dialogState === 'ReadyForFulfillment' || state.audioOutput.dialogState === 'Fulfilled' || state.audioOutput.dialogState === 'Failed' || !state.config.silenceDetection) {\n                state.transition(new Initial(state));\n              } else {\n                audioControl.startRecording(state.onSilence, state.onAudioData, state.config.silenceDetectionConfig);\n                state.transition(new Listening(state));\n              }\n            });\n          } else {\n            state.transition(new Initial(state));\n          }\n        };\n      };\n\n      var applyDefaults = function applyDefaults(config) {\n        config = config || {};\n        config.silenceDetection = config.hasOwnProperty('silenceDetection') ? config.silenceDetection : true;\n        var lexConfig = config.lexConfig || {};\n        lexConfig.botAlias = lexConfig.hasOwnProperty('botAlias') ? lexConfig.botAlias : DEFAULT_LATEST;\n        lexConfig.botName = lexConfig.hasOwnProperty('botName') ? lexConfig.botName : '';\n        lexConfig.contentType = lexConfig.hasOwnProperty('contentType') ? lexConfig.contentType : DEFAULT_CONTENT_TYPE;\n        lexConfig.userId = lexConfig.hasOwnProperty('userId') ? lexConfig.userId : DEFAULT_USER_ID;\n        lexConfig.accept = lexConfig.hasOwnProperty('accept') ? lexConfig.accept : DEFAULT_ACCEPT_HEADER_VALUE;\n        config.lexConfig = lexConfig;\n        return config;\n      };\n    })();\n  }, {\n    \"./control.js\": 1\n  }],\n  3: [function (require, module, exports) {\n    (function (global) {\n      /**\n       * @module LexAudio\n       * @description The global namespace for Amazon Lex Audio\n       */\n      global.LexAudio = global.LexAudio || {};\n      global.LexAudio.audioControl = require('./control.js').audioControl;\n      global.LexAudio.conversation = require('./conversation.js').conversation;\n      module.exports = global.LexAudio;\n    }).call(this, typeof global !== \"undefined\" ? global : typeof self !== \"undefined\" ? self : typeof window !== \"undefined\" ? window : {});\n  }, {\n    \"./control.js\": 1,\n    \"./conversation.js\": 2\n  }],\n  4: [function (require, module, exports) {\n    var bundleFn = arguments[3];\n    var sources = arguments[4];\n    var cache = arguments[5];\n    var stringify = JSON.stringify;\n\n    module.exports = function (fn, options) {\n      var wkey;\n      var cacheKeys = Object.keys(cache);\n\n      for (var i = 0, l = cacheKeys.length; i < l; i++) {\n        var key = cacheKeys[i];\n        var exp = cache[key].exports; // Using babel as a transpiler to use esmodule, the export will always\n        // be an object with the default export as a property of it. To ensure\n        // the existing api and babel esmodule exports are both supported we\n        // check for both\n\n        if (exp === fn || exp && exp.default === fn) {\n          wkey = key;\n          break;\n        }\n      }\n\n      if (!wkey) {\n        wkey = Math.floor(Math.pow(16, 8) * Math.random()).toString(16);\n        var wcache = {};\n\n        for (var i = 0, l = cacheKeys.length; i < l; i++) {\n          var key = cacheKeys[i];\n          wcache[key] = key;\n        }\n\n        sources[wkey] = [Function(['require', 'module', 'exports'], '(' + fn + ')(self)'), wcache];\n      }\n\n      var skey = Math.floor(Math.pow(16, 8) * Math.random()).toString(16);\n      var scache = {};\n      scache[wkey] = wkey;\n      sources[skey] = [Function(['require'], // try to call default if defined to also support babel esmodule\n      // exports\n      'var f = require(' + stringify(wkey) + ');' + '(f.default ? f.default : f)(self);'), scache];\n      var workerSources = {};\n      resolveSources(skey);\n\n      function resolveSources(key) {\n        workerSources[key] = true;\n\n        for (var depPath in sources[key][1]) {\n          var depKey = sources[key][1][depPath];\n\n          if (!workerSources[depKey]) {\n            resolveSources(depKey);\n          }\n        }\n      }\n\n      var src = '(' + bundleFn + ')({' + Object.keys(workerSources).map(function (key) {\n        return stringify(key) + ':[' + sources[key][0] + ',' + stringify(sources[key][1]) + ']';\n      }).join(',') + '},{},[' + stringify(skey) + '])';\n      var URL = window.URL || window.webkitURL || window.mozURL || window.msURL;\n      var blob = new Blob([src], {\n        type: 'text/javascript'\n      });\n\n      if (options && options.bare) {\n        return blob;\n      }\n\n      var workerUrl = URL.createObjectURL(blob);\n      var worker = new Worker(workerUrl);\n      worker.objectURL = workerUrl;\n      return worker;\n    };\n  }, {}],\n  5: [function (require, module, exports) {\n    (function () {\n      'use strict';\n\n      var work = require('webworkify');\n\n      var worker = work(require('./worker.js'));\n      var audio_context, audio_stream;\n      /**\n       * The Recorder object. Sets up the onaudioprocess callback and communicates\n       * with the web worker to perform audio actions.\n       */\n\n      var recorder = function recorder(source, silenceDetectionConfig) {\n        silenceDetectionConfig = silenceDetectionConfig || {};\n        silenceDetectionConfig.time = silenceDetectionConfig.hasOwnProperty('time') ? silenceDetectionConfig.time : 1500;\n        silenceDetectionConfig.amplitude = silenceDetectionConfig.hasOwnProperty('amplitude') ? silenceDetectionConfig.amplitude : 0.2;\n        var recording = false,\n            currCallback,\n            start,\n            silenceCallback,\n            visualizationCallback; // Create a ScriptProcessorNode with a bufferSize of 4096 and a single input and output channel\n\n        var node = source.context.createScriptProcessor(4096, 1, 1);\n\n        worker.onmessage = function (message) {\n          var blob = message.data;\n          currCallback(blob);\n        };\n\n        worker.postMessage({\n          command: 'init',\n          config: {\n            sampleRate: source.context.sampleRate\n          }\n        });\n        /**\n         * Sets the silence and viz callbacks, resets the silence start time, and sets recording to true.\n         * @param {?onSilenceCallback} onSilence - Called when silence is detected.\n         * @param {?visualizerCallback} visualizer - Can be used to visualize the captured buffer.\n         */\n\n        var record = function record(onSilence, visualizer) {\n          silenceCallback = onSilence;\n          visualizationCallback = visualizer;\n          start = Date.now();\n          recording = true;\n        };\n        /**\n         * Sets recording to false.\n         */\n\n\n        var stop = function stop() {\n          recording = false;\n        };\n        /**\n         * Posts \"clear\" message to the worker.\n         */\n\n\n        var clear = function clear() {\n          stop();\n          worker.postMessage({\n            command: 'clear'\n          });\n        };\n        /**\n         * Sets the export callback and posts an \"export\" message to the worker.\n         * @param {onExportComplete} callback - Called when the export is complete.\n         * @param {sampleRate} The sample rate to use in the export.\n         */\n\n\n        var exportWAV = function exportWAV(callback, sampleRate) {\n          currCallback = callback;\n          worker.postMessage({\n            command: 'export',\n            sampleRate: sampleRate\n          });\n        };\n        /**\n         * Checks the time domain data to see if the amplitude of the audio waveform is more than\n         * the silence threshold. If it is, \"noise\" has been detected and it resets the start time.\n         * If the elapsed time reaches the time threshold the silence callback is called. If there is a\n         * visualizationCallback it invokes the visualization callback with the time domain data.\n         */\n\n\n        var analyse = function analyse() {\n          analyser.fftSize = 2048;\n          var bufferLength = analyser.fftSize;\n          var dataArray = new Uint8Array(bufferLength);\n          var amplitude = silenceDetectionConfig.amplitude;\n          var time = silenceDetectionConfig.time;\n          analyser.getByteTimeDomainData(dataArray);\n\n          if (typeof visualizationCallback === 'function') {\n            visualizationCallback(dataArray, bufferLength);\n          }\n\n          for (var i = 0; i < bufferLength; i++) {\n            // Normalize between -1 and 1.\n            var curr_value_time = dataArray[i] / 128 - 1.0;\n\n            if (curr_value_time > amplitude || curr_value_time < -1 * amplitude) {\n              start = Date.now();\n            }\n          }\n\n          var newtime = Date.now();\n          var elapsedTime = newtime - start;\n\n          if (elapsedTime > time) {\n            silenceCallback();\n          }\n        };\n        /**\n         * The onaudioprocess event handler of the ScriptProcessorNode interface. It is the EventHandler to be\n         * called for the audioprocess event that is dispatched to ScriptProcessorNode node types.\n         * @param {AudioProcessingEvent} audioProcessingEvent - The audio processing event.\n         */\n\n\n        node.onaudioprocess = function (audioProcessingEvent) {\n          if (!recording) {\n            return;\n          }\n\n          worker.postMessage({\n            command: 'record',\n            buffer: [audioProcessingEvent.inputBuffer.getChannelData(0)]\n          });\n          analyse();\n        };\n\n        var analyser = source.context.createAnalyser();\n        analyser.minDecibels = -90;\n        analyser.maxDecibels = -10;\n        analyser.smoothingTimeConstant = 0.85;\n        source.connect(analyser);\n        analyser.connect(node);\n        node.connect(source.context.destination);\n        return {\n          record: record,\n          stop: stop,\n          clear: clear,\n          exportWAV: exportWAV\n        };\n      };\n      /**\n       * Audio recorder object. Handles setting up the audio context,\n       * accessing the mike, and creating the Recorder object.\n       */\n\n\n      exports.audioRecorder = function () {\n        /**\n         * Creates an audio context and calls getUserMedia to request the mic (audio).\n         */\n        var requestDevice = function requestDevice() {\n          if (typeof audio_context === 'undefined') {\n            window.AudioContext = window.AudioContext || window.webkitAudioContext;\n            audio_context = new AudioContext();\n          }\n\n          return navigator.mediaDevices.getUserMedia({\n            audio: true\n          }).then(function (stream) {\n            audio_stream = stream;\n          });\n        };\n\n        var createRecorder = function createRecorder(silenceDetectionConfig) {\n          return recorder(audio_context.createMediaStreamSource(audio_stream), silenceDetectionConfig);\n        };\n\n        var audioContext = function audioContext() {\n          return audio_context;\n        };\n\n        return {\n          requestDevice: requestDevice,\n          createRecorder: createRecorder,\n          audioContext: audioContext\n        };\n      };\n    })();\n  }, {\n    \"./worker.js\": 6,\n    \"webworkify\": 4\n  }],\n  6: [function (require, module, exports) {\n    module.exports = function (self) {\n      'use strict';\n\n      var recLength = 0,\n          recBuffer = [],\n          recordSampleRate;\n      self.addEventListener('message', function (e) {\n        switch (e.data.command) {\n          case 'init':\n            init(e.data.config);\n            break;\n\n          case 'record':\n            record(e.data.buffer);\n            break;\n\n          case 'export':\n            exportBuffer(e.data.sampleRate);\n            break;\n\n          case 'clear':\n            clear();\n            break;\n        }\n      });\n\n      function init(config) {\n        recordSampleRate = config.sampleRate;\n      }\n\n      function record(inputBuffer) {\n        recBuffer.push(inputBuffer[0]);\n        recLength += inputBuffer[0].length;\n      }\n\n      function exportBuffer(exportSampleRate) {\n        var mergedBuffers = mergeBuffers(recBuffer, recLength);\n        var downsampledBuffer = downsampleBuffer(mergedBuffers, exportSampleRate);\n        var encodedWav = encodeWAV(downsampledBuffer);\n        var audioBlob = new Blob([encodedWav], {\n          type: 'application/octet-stream'\n        });\n        postMessage(audioBlob);\n      }\n\n      function clear() {\n        recLength = 0;\n        recBuffer = [];\n      }\n\n      function downsampleBuffer(buffer, exportSampleRate) {\n        if (exportSampleRate === recordSampleRate) {\n          return buffer;\n        }\n\n        var sampleRateRatio = recordSampleRate / exportSampleRate;\n        var newLength = Math.round(buffer.length / sampleRateRatio);\n        var result = new Float32Array(newLength);\n        var offsetResult = 0;\n        var offsetBuffer = 0;\n\n        while (offsetResult < result.length) {\n          var nextOffsetBuffer = Math.round((offsetResult + 1) * sampleRateRatio);\n          var accum = 0,\n              count = 0;\n\n          for (var i = offsetBuffer; i < nextOffsetBuffer && i < buffer.length; i++) {\n            accum += buffer[i];\n            count++;\n          }\n\n          result[offsetResult] = accum / count;\n          offsetResult++;\n          offsetBuffer = nextOffsetBuffer;\n        }\n\n        return result;\n      }\n\n      function mergeBuffers(bufferArray, recLength) {\n        var result = new Float32Array(recLength);\n        var offset = 0;\n\n        for (var i = 0; i < bufferArray.length; i++) {\n          result.set(bufferArray[i], offset);\n          offset += bufferArray[i].length;\n        }\n\n        return result;\n      }\n\n      function floatTo16BitPCM(output, offset, input) {\n        for (var i = 0; i < input.length; i++, offset += 2) {\n          var s = Math.max(-1, Math.min(1, input[i]));\n          output.setInt16(offset, s < 0 ? s * 0x8000 : s * 0x7FFF, true);\n        }\n      }\n\n      function writeString(view, offset, string) {\n        for (var i = 0; i < string.length; i++) {\n          view.setUint8(offset + i, string.charCodeAt(i));\n        }\n      }\n\n      function encodeWAV(samples) {\n        var buffer = new ArrayBuffer(44 + samples.length * 2);\n        var view = new DataView(buffer);\n        writeString(view, 0, 'RIFF');\n        view.setUint32(4, 32 + samples.length * 2, true);\n        writeString(view, 8, 'WAVE');\n        writeString(view, 12, 'fmt ');\n        view.setUint32(16, 16, true);\n        view.setUint16(20, 1, true);\n        view.setUint16(22, 1, true);\n        view.setUint32(24, recordSampleRate, true);\n        view.setUint32(28, recordSampleRate * 2, true);\n        view.setUint16(32, 2, true);\n        view.setUint16(34, 16, true);\n        writeString(view, 36, 'data');\n        view.setUint32(40, samples.length * 2, true);\n        floatTo16BitPCM(view, 44, samples);\n        return view;\n      }\n    };\n  }, {}]\n}, {}, [3]);","map":null,"metadata":{},"sourceType":"script"}